{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d476f60c-f0e2-4c80-a6cb-bd3aa00c1d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy'] = \"http://192.41.170.23:3128\" \n",
    "os.environ['https_proxy'] = \"http://192.41.170.23:3128\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d80263e2-31a9-4d26-aaa5-8fab5d404336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pycocotools\n",
    "# !pip install faiss-cpu faiss-gpu\n",
    "# !pip install nltk\n",
    "# !pip install salesforce-lavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e796c24f-aed0-4b3f-9876-32991761c09d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import model.clip as clip\n",
    "from model.model import ResidualAttentionBlock\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, PILToTensor\n",
    "import cv2\n",
    "from captum.attr import visualization\n",
    "from tqdm import tqdm\n",
    "from transformers import ViTForImageClassification\n",
    "import timm\n",
    "import os\n",
    "import skimage\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "from faiss import write_index, read_index\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torchvision.datasets import CocoCaptions\n",
    "\n",
    "from collections import OrderedDict\n",
    "from datasets import load_dataset\n",
    "import faiss\n",
    "import gc\n",
    "from typing import Any, Tuple, Callable, Optional, List\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# from lavis.models import model_zoo, load_model_and_preprocess\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "cuda_card = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc36b416-0420-428b-963c-996af5a37aa0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 149,620,737\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-L/14\", get_all_token=False)\n",
    "model.cuda(cuda_card).eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bb0e738-b273-438f-8a87-36e70ce5abea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_vector(arr):\n",
    "    norms = torch.linalg.norm(arr, axis=1, keepdims=True)\n",
    "    return arr / norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51536f7c-247a-4810-aa52-78f15d848807",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoCustom(CocoCaptions): \n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        annFile: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "        transforms: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(root, annFile, transform, target_transform, transforms)\n",
    "        from pycocotools.coco import COCO\n",
    "\n",
    "        self.annotations = json.load(open(annFile))\n",
    "        self.num_captions = len(self.annotations['annotations'])\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        id = self.ids[index]\n",
    "        image = self._load_image(id)\n",
    "        target = self._load_target(id)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image, target = self.transforms(image, target)\n",
    "\n",
    "        return image, id, target\n",
    "\n",
    "    def getAnnotationRange(self, index: int, count: int) -> List[Any]:\n",
    "        return [self.annotations['annotations'][index]['caption'] for index in range(index, index+count)]\n",
    "\n",
    "    def getImgIdFromAnnotationIndex(self, annotation_index: int) -> int:\n",
    "        return self.annotations['annotations'][annotation_index]['image_id']\n",
    "    \n",
    "    def buildFaissIndex(self, text_encoder, tokenize, batch_size, nlist) :\n",
    "        tokenized = tokenize(self.getAnnotationRange(0, batch_size)).cuda(cuda_card)\n",
    "        encoded_captions = normalize_vector(text_encoder(tokenized, get_all_token=False).detach().cpu().numpy().astype('float32'))\n",
    "        vector_dimension = encoded_captions.shape[1]\n",
    "        \n",
    "        quantizer = faiss.IndexFlatIP(vector_dimension)\n",
    "        index = faiss.IndexIVFFlat(quantizer, vector_dimension, nlist)\n",
    "        index.train(encoded_captions)\n",
    "        index.add(encoded_captions)\n",
    "        \n",
    "        for i in tqdm(range(batch_size, self.num_captions - batch_size, batch_size)):\n",
    "            tokenized = clip.tokenize(self.getAnnotationRange(i, batch_size)).cuda(cuda_card)\n",
    "            encoded_captions = normalize_vector(model.encode_text(tokenized, get_all_token=False).detach().cpu().numpy().astype('float32'))\n",
    "            index.add(encoded_captions)\n",
    "\n",
    "        return index\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b812476-770a-49bf-9d2f-dec9011b5e62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of samples:  5000\n",
      "Image Size: torch.Size([3, 224, 224])\n",
      "Captions: [\"A stop sign is mounted upside-down on it's post. \", 'A stop sign that is hanging upside down.', 'An upside down stop sign by the road.', 'a stop sign put upside down on a metal pole ', 'A stop sign installed upside down on a street corner']\n",
      "Image Id: 724\n"
     ]
    }
   ],
   "source": [
    "path = '../../Dataset/CV/mscoco/2017'\n",
    "cocoCaptions = CocoCustom(root = path + '/val2017',\n",
    "                        annFile = path + '/annotations/captions_val2017.json',\n",
    "                        transform=preprocess)\n",
    "\n",
    "print('Number of samples: ', len(cocoCaptions))\n",
    "img, img_id, target = cocoCaptions[3]\n",
    "\n",
    "print(\"Image Size:\", img.size())\n",
    "print(\"Captions:\", target)\n",
    "print(\"Image Id:\", img_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d33b5782-c627-40bb-bccb-6f6c923c9f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [06:07<00:00, 13.61it/s]\n"
     ]
    }
   ],
   "source": [
    "caption_map = {}\n",
    "captions = []\n",
    "caption_idx = 0\n",
    "images_id_list = []\n",
    "for i in tqdm(range(len(cocoCaptions))):\n",
    "    img, img_id, target = cocoCaptions[i]\n",
    "    images_id_list.append(img_id)\n",
    "    for idx in range(len(target)):\n",
    "        capt = target[idx]\n",
    "        caption_map[caption_idx] = img_id\n",
    "        caption_idx += 1\n",
    "        captions.append(capt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c95f19b-8775-41fe-8626-74c4dc50eeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:49<00:00, 15.73it/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:' + str(cuda_card))\n",
    "batch_size = 32\n",
    "encoded_captions = []\n",
    "\n",
    "for i in tqdm(range(0, len(captions), batch_size)):\n",
    "    text = captions[i: min(len(captions), i+batch_size)]\n",
    "    tokenized = clip.tokenize(text).to(device)\n",
    "    encoded = normalize_vector(model.encode_text(tokenized, get_all_token=False).detach())\n",
    "    encoded_captions.append(encoded)\n",
    "encoded_captions = torch.concat(encoded_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f336dbb-5eaf-4617-b56f-08975acb4de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa7344bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_coco(data):\n",
    "    images = torch.stack([image for image, image_id, text in data])\n",
    "    images_id = torch.tensor([image_id for image, image_id, text in data])\n",
    "    return images, images_id\n",
    "\n",
    "batch_size = 50\n",
    "data_loader = DataLoader(cocoCaptions, collate_fn=collate_coco, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd6a0e7d-25f9-492d-8a42-fdadb745bf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [05:45<00:00,  3.45s/it]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "predicted = []\n",
    "score_matrix_i2t = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, images_id in tqdm(data_loader):\n",
    "        images = images.cuda(cuda_card)\n",
    "        image_encodes = normalize_vector(model.encode_image(images).detach())\n",
    "        similarity_score = image_encodes.matmul(encoded_captions.T)\n",
    "        score_matrix_i2t.append(similarity_score)\n",
    "        # _, indexes = faissIndex.search(normalize_vector(image_encodes.cpu().numpy().astype(np.float32)), 1)\n",
    "        # predicted_image_id = [caption_map[int(predicted.cpu())] for predicted in torch.argmax(similarity_score, dim=1)]\n",
    "        # predicted.extend(predicted_image_id)\n",
    "        # ground_truth.extend(images_id.tolist())\n",
    "score_matrix_i2t = torch.concat(score_matrix_i2t).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9dcdaee-e615-4972-8b3e-ce0739488479",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2txt = dict([(value, []) for value in images_id_list])\n",
    "for key, value in caption_map.items():\n",
    "    img2txt[value].append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd6ad323-06c9-47ae-b92e-6eeb248a8075",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = np.zeros(score_matrix_i2t.shape[0])\n",
    "for index, score in enumerate(score_matrix_i2t):\n",
    "    score = np.array(score)\n",
    "    inds = np.flip(np.argsort(score))\n",
    "    # Score\n",
    "    rank = 1e20\n",
    "    for i in img2txt[images_id_list[index]]:\n",
    "        tmp = np.where(inds == i)[0] # search for text i location in score array tmp is single index number\n",
    "        if len(tmp) != 0: \n",
    "            tmp = tmp[0]\n",
    "        else:\n",
    "            tmp = 1e20\n",
    "        if tmp < rank:\n",
    "            rank = tmp\n",
    "    ranks[index] = rank\n",
    "\n",
    "# Compute metrics\n",
    "tr1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n",
    "tr5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9afa18f-6bb5-4396-ac92-20be2f3de77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.82"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c96cf60d-c0c8-486b-95d2-6a2005136d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.68"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f13799f-ab2d-4701-b620-7892e66f94f8",
   "metadata": {},
   "source": [
    "# Cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1da2ee-4956-403c-bdf0-2cf92475b0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('cifar10')\n",
    "train_ds = ds['train']\n",
    "test_ds = ds['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f642fd0a-1fd8-4de0-a0cb-1992971bc5b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Train dataset: ', train_ds)\n",
    "print('Test dataset: ', test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6908369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict_key = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455522a5-c1f9-4bcf-bc1e-7fc06523f68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {id:label.replace('_', ' ') for id, label in enumerate(train_ds.features[label_dict_key].names)}\n",
    "label2id = {label:id for id,label in id2label.items()}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc016cb-1731-4924-bd07-6579bef2eb25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_transforms(examples):\n",
    "    examples['pixel_values'] = [preprocess(image.convert(\"RGB\")) for image in examples['img']]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c61fc12-4561-4df8-96f6-617d1231e98e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds.set_transform(train_transforms)\n",
    "test_ds.set_transform(train_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae371c6-154d-4036-9d4e-1964d8e5a8ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[label_dict_key] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "batch_size = 60\n",
    "train_dataloader = DataLoader(train_ds, collate_fn=collate_fn, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_ds, collate_fn=collate_fn, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3764bfd2-bb29-432f-b984-a912f8332057",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_prompted = []\n",
    "for label_id, label_name in id2label.items():\n",
    "    label_prompted.append('This is an image of ' + label_name)\n",
    "\n",
    "tokenized = clip.tokenize(label_prompted).cuda(cuda_card)\n",
    "encoded_label = model.encode_text(tokenized, get_all_token=False).detach()\n",
    "encoded_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9f3608-980a-4310-a9c2-0f38fe037092",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6eb9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossModalClassifier(nn.Module):\n",
    "    \"\"\"Linear layer to train on top of frozen features\"\"\"\n",
    "    def __init__(self, embed_dim, n_token, num_class, num_heads=1):\n",
    "        super(CrossModalClassifier, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_class = num_class\n",
    "        self.n_token = n_token\n",
    "        self.cross_att = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.linear = nn.Linear(embed_dim * n_token, num_class)\n",
    "\n",
    "    def forward(self, image_emb, text_emb):\n",
    "        x, _ = self.cross_att(text_emb, image_emb, image_emb)\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e10c0242",
   "metadata": {},
   "source": [
    "## CLIP Similarity matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd58f8-f5f1-462f-96f1-43af2bef7ce8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_count = 0\n",
    "correct_count = 0\n",
    "sum_score = torch.zeros(encoded_label.shape[0]).cuda(cuda_card)\n",
    "count_class = torch.zeros(encoded_label.shape[0]).cuda(cuda_card)\n",
    "score_per_image = [[]for i in range(encoded_label.shape[0])]\n",
    "\n",
    "# with torch.backends.cuda.sdp_kernel(enable_flash=True) as disable:\n",
    "for mini_batch in tqdm(test_dataloader):\n",
    "    images = mini_batch['pixel_values'].cuda(cuda_card)\n",
    "    labels = mini_batch['labels']\n",
    "    images_encode = model.encode_image(images)\n",
    "    norm = torch.norm(images_encode, dim=1, keepdim=True) @ torch.norm(encoded_label, dim=1, keepdim=True).T\n",
    "    images_encode = images_encode.detach()\n",
    "    \n",
    "    max_similarity = torch.max((images_encode @ encoded_label.T) / norm, dim=1)\n",
    "    predicted = max_similarity.indices\n",
    "    max_score = max_similarity.values.detach()\n",
    "    \n",
    "    correct_count += (predicted.cpu() == labels).sum()\n",
    "    sample_count += len(labels)\n",
    "    \n",
    "    for index, predict_class in enumerate(predicted):\n",
    "        score_per_image[predict_class].append(max_score[index])\n",
    "        sum_score[predict_class] += max_score[index]\n",
    "        count_class[predict_class] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6bae40-0b33-466c-8c3b-c682e6811d13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_per_class = sum_score/count_class\n",
    "print(\"Average Score: \", torch.mean(avg_per_class))\n",
    "print(avg_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ba312f-c9b9-4997-bf1d-37e367f69e6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy = correct_count / sample_count * 100\n",
    "print('Accuracy score: ', accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f30e66a",
   "metadata": {},
   "source": [
    "## Train CrossModal Classifier layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67361bda-a57e-4b48-9cbd-800b530d731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossAttLinearClassifier = CrossModalClassifier(encoded_label.shape[2], encoded_label.shape[1], encoded_label.shape[0]).cuda(cuda_card)\n",
    "# optimizer = torch.optim.SGD(\n",
    "#     crossAttLinearClassifier.parameters(),\n",
    "#     lr=0.05,\n",
    "#     momentum=0.9,\n",
    "#     weight_decay=0, # we do not apply weight decay\n",
    "# )\n",
    "# ce_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfa2a01-55d0-4fa1-95ba-c228c6ccf61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batches_per_epoch = len(train_dataloader)\n",
    "# n_epochs = 5\n",
    "# # with torch.backends.cuda.sdp_kernel(enable_flash=False) as disable:\n",
    "# for epoch in range(n_epochs):\n",
    "#     with tqdm(train_dataloader, unit=\"batch\") as tepoch:\n",
    "#         tepoch.set_description(f\"Epoch {epoch}/{n_epochs}\")\n",
    "\n",
    "#         crossAttLinearClassifier.train()\n",
    "#         for mini_batch in tepoch:\n",
    "#             images = mini_batch['pixel_values'].cuda(cuda_card)\n",
    "#             labels = mini_batch['labels'].cuda(cuda_card)\n",
    "            \n",
    "#             text_encode = []\n",
    "#             for i in labels:\n",
    "#                 text_encode.append(encoded_label[i])\n",
    "#             text_encode = torch.stack(text_encode).cuda(cuda_card).float()\n",
    "#             with torch.no_grad():\n",
    "#                 images_encode = model.encode_image(images)\n",
    "#                 images_encode = images_encode.float()\n",
    "#             output = crossAttLinearClassifier(images_encode, text_encode)\n",
    "#             predictions = output.argmax(dim=1, keepdim=True).squeeze()\n",
    "#             loss = ce_loss(output, labels)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             correct = (predictions == labels).sum().item()\n",
    "#             accuracy = correct / batch_size * 100\n",
    "#             tepoch.set_postfix(loss=loss.item(), accuracy=accuracy)\n",
    "\n",
    "# torch.save(crossAttLinearClassifier.state_dict(), './weight/crossAttention')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "659e10e5",
   "metadata": {},
   "source": [
    "## Fine tune with CrossModal Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ba60f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossAttLinearClassifier = CrossModalClassifier(encoded_label.shape[2], encoded_label.shape[1], encoded_label.shape[0]).cuda(cuda_card)\n",
    "# crossAttLinearClassifier.load_state_dict(torch.load('./weight/crossAttention'))\n",
    "\n",
    "# # vitClassifier = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "# # vitClassifier.classifier = nn.Linear(768, 10)\n",
    "# # vitClassifier.cuda(cuda_card)\n",
    "\n",
    "# vit = timm.create_model('vit_small_patch16_224.dino', pretrained=True)\n",
    "# vit.cuda(cuda_card)\n",
    "# vitClassifier = nn.Linear(384, 10).cuda(cuda_card)\n",
    "# # vitClassifier = nn.Sequential(vit, vitClassifier)\n",
    "\n",
    "# print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in vit.parameters()]):,}\")\n",
    "\n",
    "# optimizer = torch.optim.SGD(\n",
    "#     vitClassifier.parameters(),\n",
    "#     lr=0.05,\n",
    "#     momentum=0.9,\n",
    "#     weight_decay=0, # we do not apply weight decay\n",
    "# )\n",
    "# ce_loss = nn.CrossEntropyLoss()\n",
    "# softmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ef736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batches_per_epoch = len(train_dataloader)\n",
    "# n_epochs = 5\n",
    "# # with torch.backends.cuda.sdp_kernel(enable_flash=False) as disable:\n",
    "# crossAttLinearClassifier.eval()\n",
    "# model.eval()\n",
    "# for epoch in range(n_epochs):\n",
    "#     with tqdm(train_dataloader, unit=\"batch\") as tepoch:\n",
    "#         tepoch.set_description(f\"Epoch {epoch}/{n_epochs}\")\n",
    "#         vit.eval()\n",
    "#         vitClassifier.train()\n",
    "#         for mini_batch in tepoch:\n",
    "#             images = mini_batch['pixel_values'].cuda(cuda_card)\n",
    "#             labels = mini_batch['labels'].cuda(cuda_card)\n",
    "#             text_encode = []\n",
    "#             for i in labels:\n",
    "#                 text_encode.append(encoded_label[i])\n",
    "#             text_encode = torch.stack(text_encode).cuda(cuda_card).float().detach()\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 images_encode = model.encode_image(images)\n",
    "#                 images_encode = images_encode.float().detach()\n",
    "                \n",
    "#             target = softmax(crossAttLinearClassifier(images_encode, text_encode).detach())\n",
    "#             # output = softmax(vitClassifier(images))\n",
    "            \n",
    "#             vit_output = vit(images).detach()\n",
    "#             output = softmax(vitClassifier(vit_output))\n",
    "            \n",
    "#             predictions = output.argmax(dim=1, keepdim=True).squeeze()\n",
    "            \n",
    "#             loss = ce_loss(output, target)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             correct = (predictions == labels).sum().item()\n",
    "#             accuracy = correct / batch_size * 100\n",
    "#             tepoch.set_postfix(loss=loss.item(), accuracy=accuracy)\n",
    "#         torch.save(vitClassifier.state_dict(), './weight/vitClassifier{}'.format(epoch))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53449bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_count = 0\n",
    "# correct_count = 0\n",
    "\n",
    "# vitClassifier.eval()\n",
    "# vit.eval()\n",
    "# for mini_batch in tqdm(test_dataloader):\n",
    "#     images = mini_batch['pixel_values'].cuda(cuda_card)\n",
    "#     labels = mini_batch['labels'].cuda(cuda_card)\n",
    "\n",
    "#     vit_output = vit(images).detach()\n",
    "#     output = softmax(vitClassifier(vit_output).detach())\n",
    "#     predictions = output.argmax(dim=1, keepdim=True).squeeze()\n",
    "    \n",
    "#     correct_count += (predictions == labels).sum()\n",
    "#     sample_count += len(labels)\n",
    "\n",
    "# accuracy = correct_count / sample_count * 100\n",
    "# print('Accuracy score: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b25408-0c44-43fc-99ca-e1561cf1d979",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "# for param in vit.parameters():\n",
    "#     param.requires_grad = False\n",
    "vit.classifier = nn.Linear(768, 10)\n",
    "vit.classifier.weight.requires_grad = True\n",
    "vit.classifier.bias.requires_grad = True\n",
    "vit.cuda(cuda_card)\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in vit.parameters()]):,}\")\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    vit.parameters(),\n",
    "    lr=0.05,\n",
    "    # momentum=0.9,\n",
    "    weight_decay=0, # we do not apply weight decay\n",
    ")\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "softmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b96551-b4a9-45a7-ae28-6c41e1b4b026",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch = len(train_dataloader)\n",
    "n_epochs = 10\n",
    "# with torch.backends.cuda.sdp_kernel(enable_flash=False) as disable:\n",
    "vit.train()\n",
    "for epoch in range(n_epochs):\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:\n",
    "        tepoch.set_description(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "        for mini_batch in tepoch:\n",
    "            images = mini_batch['pixel_values'].cuda(cuda_card)\n",
    "            labels = mini_batch['labels'].cuda(cuda_card)\n",
    "            \n",
    "            output = softmax(vit(images).logits)\n",
    "            \n",
    "            predictions = output.argmax(dim=1, keepdim=True).squeeze()\n",
    "            \n",
    "            loss = ce_loss(output, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            correct = (predictions == labels).sum().item()\n",
    "            accuracy = correct / batch_size * 100\n",
    "            tepoch.set_postfix(loss=loss.item(), accuracy=accuracy)\n",
    "        torch.save(vit.state_dict(), './weight_supervised/vitBaseClassifierNoFreeze{}'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e82fd05-8720-4a53-9c58-177ab7b7fc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_count = 0\n",
    "correct_count = 0\n",
    "\n",
    "vit.eval()\n",
    "for mini_batch in tqdm(test_dataloader):\n",
    "    images = mini_batch['pixel_values'].cuda(cuda_card)\n",
    "    labels = mini_batch['labels'].cuda(cuda_card)\n",
    "\n",
    "    output = softmax(vit(images).logits.detach())\n",
    "    predictions = output.argmax(dim=1, keepdim=True).squeeze()\n",
    "    \n",
    "    correct_count += (predictions == labels).sum()\n",
    "    sample_count += len(labels)\n",
    "\n",
    "accuracy = correct_count / sample_count * 100\n",
    "print('Accuracy score: ', accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df9b1148-1ef1-4734-a0e6-d76d099ace32",
   "metadata": {},
   "source": [
    "# Pototypical Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596819b8-f928-4a0c-ba29-513b088a4908",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prototypes = torch.ones(encoded_label.shape, dtype=torch.float16).cuda(cuda_card)\n",
    "# check_class = torch.zeros(encoded_label.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4706ffcf-8d76-416d-a2e3-8a252737ec19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prototype_weight = 0.85\n",
    "# for mini_batch in tqdm(train_dataloader):\n",
    "#     images = mini_batch['pixel_values'].cuda(cuda_card)\n",
    "#     labels = mini_batch['labels']\n",
    "#     images_encode = model.encode_image(images)\n",
    "\n",
    "#     for index, image_encode in enumerate(images_encode.detach()):\n",
    "#         class_no = labels[index]\n",
    "#         if check_class[class_no] == 0:\n",
    "#             prototypes[class_no] = image_encode\n",
    "#             check_class[class_no] = 1\n",
    "#         else:\n",
    "#             prototypes[class_no] = (prototypes[class_no] * prototype_weight) + (image_encode * (1 - prototype_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f25fb9-7f8c-4d3a-afbb-8ea2b970d140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0402007f-6577-46b4-bb03-ecb87794d8d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample_count = 0\n",
    "# correct_count = 0\n",
    "# for mini_batch in tqdm(test_dataloader):\n",
    "#     images = mini_batch['pixel_values'].cuda(cuda_card)\n",
    "#     labels = mini_batch['labels'].cuda(cuda_card)\n",
    "#     images_encode = model.encode_image(images)\n",
    "    \n",
    "#     predicted = torch.argmax(images_encode @ prototypes.T, dim=1)\n",
    "#     correct_count += (predicted == labels).sum()\n",
    "#     sample_count += len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42b7a73-e55c-487e-8a53-b5b857993146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# accuracy = correct_count / sample_count * 100\n",
    "# print('Accuracy score: ', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
