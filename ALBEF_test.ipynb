{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52c82748-076e-44a7-a9d2-08554096a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy'] = \"http://192.41.170.23:3128\" \n",
    "os.environ['https_proxy'] = \"http://192.41.170.23:3128\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a608e42-9ceb-4821-8353-cb57ee349e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pycocotools in /home/pasitt/.local/lib/python3.10/site-packages (2.0.7)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools) (3.7.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pycocotools) (1.24.3)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (4.39.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (9.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ruamel.yaml in /usr/local/lib/python3.10/dist-packages (0.17.26)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from ruamel.yaml) (0.2.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pycocotools\n",
    "!pip install ruamel.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87613ba6-aefe-4b9b-896c-12a0dabc2f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ruamel.yaml as yaml\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, PILToTensor\n",
    "import cv2\n",
    "from captum.attr import visualization\n",
    "from tqdm import tqdm\n",
    "from transformers import ViTForImageClassification\n",
    "import timm\n",
    "import os\n",
    "import skimage\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torchvision.datasets import CocoCaptions\n",
    "import pycocotools\n",
    "\n",
    "from collections import OrderedDict\n",
    "from datasets import load_dataset\n",
    "import gc\n",
    "from typing import Any, Tuple, Callable, Optional, List\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from models.model_retrieval import ALBEF\n",
    "from models.vit import interpolate_pos_embed\n",
    "from models.tokenization_bert import BertTokenizer\n",
    "from torchvision import transforms\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "cuda_card = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dd52c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_vector(arr):\n",
    "    norms = torch.linalg.norm(arr, axis=1, keepdims=True)\n",
    "    return arr / norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "611932f8-dbcc-4fa2-ac95-2a1c847234ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoCustom(CocoCaptions): \n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        annFile: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "        transforms: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(root, annFile, transform, target_transform, transforms)\n",
    "        from pycocotools.coco import COCO\n",
    "\n",
    "        self.annotations = json.load(open(annFile))\n",
    "        self.num_captions = len(self.annotations['annotations'])\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        id = self.ids[index]\n",
    "        image = self._load_image(id)\n",
    "        target = self._load_target(id)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image, target = self.transforms(image, target)\n",
    "\n",
    "        return image, id, target\n",
    "\n",
    "    def getAnnotationRange(self, index: int, count: int) -> List[Any]:\n",
    "        return [self.annotations['annotations'][index]['caption'] for index in range(index, index+count)]\n",
    "\n",
    "    def getImgIdFromAnnotationIndex(self, annotation_index: int) -> int:\n",
    "        return self.annotations['annotations'][annotation_index]['image_id']\n",
    "    \n",
    "    def buildFaissIndex(self, text_encoder, tokenize, batch_size, nlist) :\n",
    "        tokenized = tokenize(self.getAnnotationRange(0, batch_size)).cuda(cuda_card)\n",
    "        encoded_captions = normalize_vector(text_encoder(tokenized, get_all_token=False).detach().cpu().numpy().astype('float32'))\n",
    "        vector_dimension = encoded_captions.shape[1]\n",
    "        \n",
    "        quantizer = faiss.IndexFlatIP(vector_dimension)\n",
    "        index = faiss.IndexIVFFlat(quantizer, vector_dimension, nlist)\n",
    "        index.train(encoded_captions)\n",
    "        index.add(encoded_captions)\n",
    "        \n",
    "        for i in tqdm(range(batch_size, self.num_captions - batch_size, batch_size)):\n",
    "            tokenized = clip.tokenize(self.getAnnotationRange(i, batch_size)).cuda(cuda_card)\n",
    "            encoded_captions = normalize_vector(model.encode_text(tokenized, get_all_token=False).detach().cpu().numpy().astype('float32'))\n",
    "            index.add(encoded_captions)\n",
    "\n",
    "        return index\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acf0e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.load(open(\"./configs/Retrieval_coco.yaml\", 'r'), Loader=yaml.Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d4e5c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = ALBEF(config=config, text_encoder=\"bert-base-uncased\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d6723e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape position embedding from 256 to 576\n",
      "reshape position embedding from 256 to 576\n",
      "_IncompatibleKeys(missing_keys=['idx_queue'], unexpected_keys=['text_encoder.cls.predictions.bias', 'text_encoder.cls.predictions.transform.dense.weight', 'text_encoder.cls.predictions.transform.dense.bias', 'text_encoder.cls.predictions.transform.LayerNorm.weight', 'text_encoder.cls.predictions.transform.LayerNorm.bias', 'text_encoder.cls.predictions.decoder.weight', 'text_encoder.cls.predictions.decoder.bias', 'text_encoder_m.cls.predictions.bias', 'text_encoder_m.cls.predictions.transform.dense.weight', 'text_encoder_m.cls.predictions.transform.dense.bias', 'text_encoder_m.cls.predictions.transform.LayerNorm.weight', 'text_encoder_m.cls.predictions.transform.LayerNorm.bias', 'text_encoder_m.cls.predictions.decoder.weight', 'text_encoder_m.cls.predictions.decoder.bias'])\n",
      "Load Model\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('./model_checkpoints/ALBEF.pth', map_location='cpu') \n",
    "state_dict = checkpoint['model']\n",
    "\n",
    "# reshape positional embedding to accomodate for image resolution change\n",
    "pos_embed_reshaped = interpolate_pos_embed(state_dict['visual_encoder.pos_embed'],model.visual_encoder)         \n",
    "state_dict['visual_encoder.pos_embed'] = pos_embed_reshaped\n",
    "m_pos_embed_reshaped = interpolate_pos_embed(state_dict['visual_encoder_m.pos_embed'],model.visual_encoder_m)   \n",
    "state_dict['visual_encoder_m.pos_embed'] = m_pos_embed_reshaped \n",
    "\n",
    "for key in list(state_dict.keys()):\n",
    "    if 'bert' in key:\n",
    "        encoder_key = key.replace('bert.','')         \n",
    "        state_dict[encoder_key] = state_dict[key] \n",
    "        del state_dict[key]                \n",
    "msg = model.load_state_dict(state_dict,strict=False)  \n",
    "\n",
    "print(msg)\n",
    "\n",
    "model = model.cuda(cuda_card)\n",
    "model.eval()\n",
    "print(\"Load Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f48985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "test_transform = transforms.Compose([\n",
    "        transforms.Resize((config['image_res'],config['image_res']),interpolation=Image.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fc1a79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of samples:  5000\n",
      "Image Size: torch.Size([3, 384, 384])\n",
      "Captions: [\"A stop sign is mounted upside-down on it's post. \", 'A stop sign that is hanging upside down.', 'An upside down stop sign by the road.', 'a stop sign put upside down on a metal pole ', 'A stop sign installed upside down on a street corner']\n",
      "Image Id: 724\n"
     ]
    }
   ],
   "source": [
    "path = '../../Dataset/CV/mscoco/2017'\n",
    "cocoCaptions = CocoCustom(root = path + '/val2017',\n",
    "                        annFile = path + '/annotations/captions_val2017.json',\n",
    "                        transform=test_transform)\n",
    "\n",
    "print('Number of samples: ', len(cocoCaptions))\n",
    "img, img_id, target = cocoCaptions[3]\n",
    "\n",
    "print(\"Image Size:\", img.size())\n",
    "print(\"Captions:\", target)\n",
    "print(\"Image Id:\", img_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c49bd772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [06:39<00:00, 12.52it/s] \n"
     ]
    }
   ],
   "source": [
    "caption_map = {}\n",
    "captions = []\n",
    "caption_idx = 0\n",
    "images_id_list = []\n",
    "for i in tqdm(range(len(cocoCaptions))):\n",
    "    img, img_id, target = cocoCaptions[i]\n",
    "    images_id_list.append(img_id)\n",
    "    for idx in range(len(target)):\n",
    "        capt = target[idx]\n",
    "        caption_map[caption_idx] = img_id\n",
    "        caption_idx += 1\n",
    "        captions.append(capt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4592c3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:' + str(cuda_card))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0dcf268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1564/1564 [00:26<00:00, 58.58it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "text_feats = []\n",
    "text_atts = []\n",
    "text_embeds = []\n",
    "\n",
    "for i in tqdm(range(0, len(captions), batch_size)):\n",
    "    text = captions[i: min(len(captions), i+batch_size)]\n",
    "    text_input = tokenizer(text, padding='max_length', truncation=True, max_length=30, return_tensors=\"pt\").to(device)\n",
    "    text_output = model.text_encoder(text_input.input_ids, attention_mask = text_input.attention_mask, mode='text')\n",
    "    text_feat = text_output.last_hidden_state.detach()\n",
    "    text_embed = model.text_proj(text_feat[:,0,:]).detach()\n",
    "    text_feats.append(text_feat)\n",
    "    text_atts.append(text_input.attention_mask)\n",
    "    text_embeds.append(normalize_vector(text_embed))\n",
    "    \n",
    "text_feats = torch.concat(text_feats)\n",
    "text_atts = torch.concat(text_atts)\n",
    "text_embeds = torch.concat(text_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14574451",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33e871cd-091a-47ee-9262-9c3ea3eb7101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_coco(data):\n",
    "    images = torch.stack([image for image, image_id, text in data])\n",
    "    images_id = torch.tensor([image_id for image, image_id, text in data])\n",
    "    return images, images_id\n",
    "\n",
    "batch_size = 50\n",
    "data_loader = DataLoader(cocoCaptions, collate_fn=collate_coco, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "140c1ae4-cd57-48dd-9f7f-5c60f05988f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:47<00:00,  4.67s/it]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct_count = 0\n",
    "total_samples = 0\n",
    "k = 120\n",
    "\n",
    "image_feats = []\n",
    "image_embeds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, images_id in tqdm(data_loader):\n",
    "        images_id_list.extend(images_id.tolist())\n",
    "        images = images.cuda(cuda_card)\n",
    "        image_feat = model.visual_encoder(images).detach()     \n",
    "        image_embed = model.vision_proj(image_feat[:,0,:]).detach()            \n",
    "        image_embed = normalize_vector(image_embed)\n",
    "\n",
    "        image_feats.append(image_feat)\n",
    "        image_embeds.append(image_embed)\n",
    "        \n",
    "image_feats = torch.concat(image_feats)\n",
    "image_embeds = torch.concat(image_embeds)\n",
    "\n",
    "sims_matrix = image_embeds.matmul(text_embeds.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39cbe823-cfc4-499e-99ec-3ca56a0f1b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 25014])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b84f2b8-b862-41f3-b822-c38703d5548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_matrix_i2t = torch.full((len(images_id_list), len(captions)), -100.0)\n",
    "score_matrix_t2i = torch.full((len(captions), len(images_id_list)), -100.0)\n",
    "\n",
    "sims_matrix = sims_matrix.t()\n",
    "\n",
    "for idx, sims in enumerate(sims_matrix):\n",
    "    _, topk_idx = sims.topk(k=k, dim=0)\n",
    "    encoder_output = image_feats[topk_idx]\n",
    "    encoder_att = torch.ones(encoder_output.size()[:-1], dtype=torch.long).to(device)\n",
    "    output = model.text_encoder(encoder_embeds = text_feats[idx].repeat(k, 1, 1), \n",
    "                                attention_mask = text_atts[idx].repeat(k, 1),\n",
    "                                encoder_hidden_states = encoder_output,\n",
    "                                encoder_attention_mask = encoder_att,                             \n",
    "                                return_dict = True,\n",
    "                                mode = 'fusion'\n",
    "                               )\n",
    "    score = score = model.itm_head(output.last_hidden_state[:,0,:])[:,1].detach().cpu()\n",
    "    score_matrix_t2i[idx, topk_idx] = score.float()\n",
    "\n",
    "sims_matrix = sims_matrix.t()\n",
    "\n",
    "for idx, sims in enumerate(sims_matrix):\n",
    "    _, topk_idx = sims.topk(k=k, dim=0)\n",
    "    encoder_output = image_feats[idx].repeat(k,1,1)\n",
    "    encoder_att = torch.ones(encoder_output.size()[:-1],dtype=torch.long).to(device)\n",
    "    output = model.text_encoder(encoder_embeds = text_feats[topk_idx], \n",
    "                                attention_mask = text_atts[topk_idx],\n",
    "                                encoder_hidden_states = encoder_output,\n",
    "                                encoder_attention_mask = encoder_att,                             \n",
    "                                return_dict = True,\n",
    "                                mode = 'fusion'\n",
    "                               )\n",
    "    score = model.itm_head(output.last_hidden_state[:,0,:])[:,1].detach().cpu()\n",
    "    score_matrix_i2t[idx, topk_idx] = score  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2478d9a-8618-4301-9fe5-4c9593a35120",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2txt = dict([(value, []) for value in images_id_list])\n",
    "for key, value in caption_map.items():\n",
    "    img2txt[value].append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "114b2276-fd61-492f-a05f-852def25f162",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_matrix_t2i = np.array(score_matrix_t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67be3f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = np.zeros(score_matrix_i2t.shape[0])\n",
    "for index, score in enumerate(score_matrix_i2t):\n",
    "    score = np.array(score)\n",
    "    inds = np.flip(np.argsort(score))\n",
    "    # Score\n",
    "    rank = 1e20\n",
    "    for i in img2txt[images_id_list[index]]:\n",
    "        tmp = np.where(inds == i)[0] # search for text i location in score array tmp is single index number\n",
    "        if len(tmp) != 0: \n",
    "            tmp = tmp[0]\n",
    "        else:\n",
    "            tmp = 1e20\n",
    "        if tmp < rank:\n",
    "            rank = tmp\n",
    "    ranks[index] = rank\n",
    "\n",
    "# Compute metrics\n",
    "tr1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n",
    "tr5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\n",
    "tr10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)\n",
    "\n",
    "ranks = np.zeros(score_matrix_t2i.shape[0])\n",
    "    \n",
    "for index,score in enumerate(score_matrix_t2i):\n",
    "    inds = np.argsort(score)[::-1]\n",
    "    tmp = np.where(inds == i)[0] # search for text i location in score array tmp is single index number\n",
    "    if len(tmp) != 0: \n",
    "        ranks[index] = np.where(inds == caption_map[index])[0][0]\n",
    "\n",
    "# Compute metrics\n",
    "ir1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n",
    "ir5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\n",
    "ir10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3aa5096-436a-4302-9a73-04e9e2ddbb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4059106b-e44f-4581-b4e9-ce53a9fee512",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"result.txt\"\n",
    "\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(\"Tr1: \" + str(tr1) + \"\\n\")\n",
    "    file.write(\"Tr5: \" + str(tr5) + \"\\n\")\n",
    "    file.write(\"Tr10: \" + str(tr10) + \"\\n\")\n",
    "    file.write(\"Ir1: \" + str(ir1) + \"\\n\")\n",
    "    file.write(\"Ir5: \" + str(ir5) + \"\\n\")\n",
    "    file.write(\"Ir10: \" + str(ir10) + \"\\n\")\n",
    "    file.write(\"Model param: \" + str(model_params) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
